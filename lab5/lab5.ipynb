{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LAB5_assignment.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Td931Q10bJen"
      },
      "source": [
        "# Bioinformatics lab 5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yKfBaYZAbMqY",
        "colab": {}
      },
      "source": [
        "from IPython.display import Image\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "plt.rcParams.update({'font.size': 18, 'axes.labelpad':'1', 'axes.titlesize' : 14})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "i5dTqksObZfP"
      },
      "source": [
        "# Training a perceptron model on Breast Cancer Wisconsin Data Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CHr2KzbBbl0f"
      },
      "source": [
        "### Loading breast cancer dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GgC5vCcPbcTn",
        "colab": {}
      },
      "source": [
        "dataset = load_breast_cancer()\n",
        "\n",
        "print(\"Whole dataset\")\n",
        "print('*'*30)\n",
        "print(\"# OF SAMPLES: {}\".format(dataset.data.shape[0]))\n",
        "print(\"# OF FEATURES: {}\".format(dataset.data.shape[1]))\n",
        "print(\"LABELS:\")\n",
        "print(dataset.target_names[0]+' corresponds to {}'.format(0))\n",
        "print(dataset.target_names[1]+' corresponds to {}'.format(1))\n",
        "print('*'*30)\n",
        "\n",
        "X = dataset.data\n",
        "y = dataset.target\n",
        "\n",
        "# Label conversion in -1 and 1\n",
        "y[np.where(dataset.target==0)] = -1\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.33, stratify=y)\n",
        "\n",
        "# Zero mean normalization\n",
        "scaler = preprocessing.StandardScaler().fit(X_train)\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test) \n",
        "\n",
        "# Visualization via pandas dataframe\n",
        "labels = np.expand_dims(y_train, axis=1)\n",
        "data_with_labels = np.concatenate((labels, X_train_scaled), axis=1)\n",
        "headers = ['labels']+dataset.feature_names.tolist()\n",
        "df = pd.DataFrame(data_with_labels, columns=headers)\n",
        "\n",
        "df.tail(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rBMe5rTxbtZ-"
      },
      "source": [
        "### Visual investigation of breast features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wdOfaoHYbw62",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots(5,6, figsize=(18,16))\n",
        "ax = ax.reshape(-1)\n",
        "for i,c in zip(np.arange(0,30), df.columns.tolist()[1:]):\n",
        "  if c != 'labels':\n",
        "    bp = df.boxplot(c, by='labels', ax=ax[i])\n",
        "plt.suptitle('')\n",
        "plt.tight_layout()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EMDvlEfnb3UG"
      },
      "source": [
        "### Features visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FmpyoUyrb8EW",
        "colab": {}
      },
      "source": [
        "def features_scatter(f1, f2, ax, tit, df):\n",
        "    \n",
        "    ax.scatter(df[f1].where(df['labels']==-1), df[f2].where(df['labels']==-1),\n",
        "            color='red', marker='o', label='malignant')\n",
        "    ax.set_xlabel(f1)\n",
        "    ax.set_ylabel(f2)\n",
        "    ax.set_title(tit)\n",
        "    \n",
        "    ax.scatter(df[f1].where(df['labels']==1), df[f2].where(df['labels']==1),\n",
        "            color='blue', marker='x', label='benign')\n",
        "\n",
        "fig, ax = plt.subplots(2, 2, figsize=(12,8))\n",
        "ax = ax.reshape(-1)\n",
        "plt.tight_layout()\n",
        "\n",
        "features_scatter('mean fractal dimension', 'smoothness error', ax[0], 'bad features', df)\n",
        "features_scatter('mean radius', 'mean concave points', ax[1], 'good features', df)\n",
        "features_scatter('compactness error', 'worst smoothness', ax[2], '', df)\n",
        "features_scatter('worst perimeter', 'worst concavity', ax[3], '', df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8t1obxnmcgkM"
      },
      "source": [
        "### Adaptive linear neurons and the convergence of learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Xo9jsdiCcjwE",
        "colab": {}
      },
      "source": [
        "class AdalineGD(object):\n",
        "    \"\"\"ADAptive LInear NEuron classifier.\n",
        "\n",
        "    Parameters\n",
        "    ------------\n",
        "    eta : float\n",
        "        Learning rate (between 0.0 and 1.0)\n",
        "    n_iter : int\n",
        "        Passes over the training dataset.\n",
        "\n",
        "    Attributes\n",
        "    -----------\n",
        "    w_ : 1d-array\n",
        "        Weights after fitting.\n",
        "    errors_ : list\n",
        "        Number of misclassifications in every epoch.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, eta=0.01, n_epochs=50):\n",
        "        self.eta = eta\n",
        "        self.n_epochs = n_epochs\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\" Fit training data.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like}, shape = [n_samples, n_features]\n",
        "            Training vectors, where n_samples is the number of samples and\n",
        "            n_features is the number of features.\n",
        "        y : array-like, shape = [n_samples]\n",
        "            Target values.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        self : object\n",
        "\n",
        "        \"\"\"\n",
        "        self.w = np.zeros(1 + X.shape[1])\n",
        "        self.cost = []\n",
        "        \n",
        "        X = np.insert(X, 0, np.ones(X.shape[0]), 1)  \n",
        "\n",
        "        for epoch in range(self.n_epochs):\n",
        "            errors = list()\n",
        "            n_features = X.shape[1]\n",
        "            for j in np.arange(0, n_features):\n",
        "                sum_e = 0\n",
        "                sum_i = 0\n",
        "                for xi, target in zip(X, y):\n",
        "                    # Start of modified section\n",
        "                    input_w = np.dot(self.w, xi)\n",
        "                    input_e = input_w - target\n",
        "                    sum_e += input_e * xi[j]\n",
        "                    # End of modified section\n",
        "                update_j = -self.eta * sum_e  # This was also modified\n",
        "                self.w[j] += update_j\n",
        "                errors.append(sum_e)               \n",
        "            cost = sum([e**2 for e in errors])/ 2.0\n",
        "            self.cost.append(cost)\n",
        "        return self\n",
        "\n",
        "    def net_input(self, X):\n",
        "        \"\"\"Calculate net input\"\"\"\n",
        "        results = np.dot(X, self.w)\n",
        "        return results\n",
        "\n",
        "    def activation(self, X):\n",
        "        \"\"\"Compute linear activation\"\"\"\n",
        "        return self.net_input(X)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Return class label\"\"\"\n",
        "        X = np.insert(X, 0, np.ones(X.shape[0]), 1) \n",
        "        return np.where(self.activation(X) >= 0.0, 1, -1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "34k-ge_Rc5yT"
      },
      "source": [
        "\n",
        "### Training and learning rate selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5Tm_lyX7c4JT",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(8, 4))\n",
        "\n",
        "# Start of modified section\n",
        "ada1 = AdalineGD(n_epochs=15, eta=0.01).fit(X_train_scaled, y_train)\n",
        "ax[0].plot(range(1, len(ada1.cost) + 1), ada1.cost, marker='o')\n",
        "ax[0].set_xlabel('Epochs')\n",
        "ax[0].set_ylabel('Sum-squared-error')\n",
        "ax[0].set_title('Adaline (%f)' % ada1.eta)\n",
        "\n",
        "ada2 = AdalineGD(n_epochs=15, eta=0.0001).fit(X_train_scaled, y_train)\n",
        "# End of modified section\n",
        "ax[1].plot(range(1, len(ada2.cost) + 1), ada2.cost, marker='o')\n",
        "ax[1].set_xlabel('Epochs')\n",
        "ax[1].set_ylabel('Sum-squared-error')\n",
        "ax[0].set_title('Adaline (%f)' % ada2.eta)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IW-TA288akHE"
      },
      "source": [
        "# Features reduction and parameters selection via cross-validation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0RBVCwcQaxZz",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import f1_score, roc_curve, auc\n",
        "\n",
        "my_cmap = matplotlib.cm.get_cmap('Set1')\n",
        "colors = list()\n",
        "for a in np.arange(0.1,1,.1):\n",
        "  colors.append(my_cmap(a))\n",
        "print(len(colors))\n",
        "\n",
        "metrics = list()\n",
        "pca_arr = list()\n",
        "ada_arr = list()\n",
        "n_components = np.arange(5,31,5)\n",
        "skf = StratifiedKFold(n_splits=len(n_components))\n",
        "i = 0\n",
        "fig, ax = plt.subplots(figsize=(10,10))\n",
        "\n",
        "for train_index, test_index in skf.split(X_train_scaled, y_train):\n",
        "    X_tr, X_ts = X_train_scaled[train_index], X_train_scaled[test_index]\n",
        "    y_tr, y_ts = y_train[train_index], y_train[test_index]\n",
        "   \n",
        "    # PCA\n",
        "    pca = PCA(n_components=n_components[i])    \n",
        "    pca.fit(X_train_scaled)\n",
        "    X_tr_pca = pca.transform(X_tr)\n",
        "    X_ts_pca = pca.transform(X_ts)\n",
        "    pca_arr.append(pca)\n",
        "\n",
        "    # model training \n",
        "    tmp_ada = AdalineGD(n_epochs=20, eta=0.0001).fit(X_tr_pca, y_tr) # Modified\n",
        "    ada_arr.append(tmp_ada)\n",
        "\n",
        "    # model evaluation\n",
        "    y_pr = tmp_ada.predict(X_ts_pca)\n",
        "\n",
        "    # metrics\n",
        "    metrics.append(f1_score(y_ts, y_pr))\n",
        "\n",
        "    # ROC curves\n",
        "    fpr, tpr, thresholds = roc_curve(y_ts, y_pr)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    ax.plot(fpr, tpr, lw=2, alpha=1, color=colors[i],\n",
        "    label='# PCA comp %d (AUC = %0.2f)' % (n_components[i], roc_auc))\n",
        "\n",
        "    i +=1\n",
        "\n",
        "ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='black', label='Luck', alpha=.8)\n",
        "ax.set_xlim([-0.1, 1.05])\n",
        "ax.set_ylim([-0.1, 1.05])\n",
        "ax.set_xlabel('False Positive Rate')\n",
        "ax.set_ylabel('True Positive Rate')\n",
        "ax.set_title('Receiver operating characteristic example')\n",
        "ax.legend(loc=\"lower right\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9tuxmqf2c_-k"
      },
      "source": [
        "### Confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NszMx8IydDgM",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "    else:\n",
        "        print('Confusion matrix without normalization')\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.grid(False)\n",
        "\n",
        "max_matrics_arg = np.argmax(metrics)\n",
        "X_test_pca = pca_arr[max_matrics_arg].transform(X_test_scaled)\n",
        "y_pred = ada_arr[max_matrics_arg].predict(X_test_pca)\n",
        "    \n",
        "# Compute confusion matrix\n",
        "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cnf_matrix, classes=dataset.target_names, normalize=True,\n",
        "                      title='Normalized confusion matrix')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}