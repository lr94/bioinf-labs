{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LAB6_assignment.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "FnUwGOj-ljBk",
        "FWCss-VqkdQ0"
      ]
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fsGeqgLtb98p"
      },
      "source": [
        "## Building a CNN  Classifier\n",
        "\n",
        "*   **Convolutional layers**, which apply a specified number of convolution\n",
        "    filters to the image. For each subregion, the layer performs a set of\n",
        "    mathematical operations to produce a single value in the output feature map.\n",
        "    Convolutional layers then typically apply a\n",
        "    [ReLU activation function](https://en.wikipedia.org/wiki/Rectifier_\\(neural_networks\\)) to\n",
        "    the output to introduce nonlinearities into the model.\n",
        "\n",
        "*   **Pooling layers**, which\n",
        "    [downsample the image data](https://en.wikipedia.org/wiki/Convolutional_neural_network#Pooling_layer)\n",
        "    extracted by the convolutional layers to reduce the dimensionality of the\n",
        "    feature map in order to decrease processing time. A commonly used pooling\n",
        "    algorithm is max pooling, which extracts subregions of the feature map\n",
        "    (e.g., 2x2-pixel tiles), keeps their maximum value, and discards all other\n",
        "    values.\n",
        "\n",
        "*   **Dense (fully connected) layers**, which perform classification on the\n",
        "    features extracted by the convolutional layers and downsampled by the\n",
        "    pooling layers. In a dense layer, every node in the layer is connected to\n",
        "    every node in the preceding layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ibXL-gFpdngL"
      },
      "source": [
        "### Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yR88-Hu8iA2g",
        "colab": {}
      },
      "source": [
        "!pip install tf-nightly-2.0-preview\n",
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "\n",
        "import cv2\n",
        "import itertools\n",
        "import numpy as np\n",
        "import datetime, os\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AWwqd8ig6G0W",
        "colab": {}
      },
      "source": [
        "# Clear any logs from previous runs\n",
        "#!rm -rf ./logs/ "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tkK1GIL4Fuwn"
      },
      "source": [
        "## Network definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d-pHh68LjRXG",
        "colab": {}
      },
      "source": [
        "class MyNet:\n",
        "    \n",
        "    def __init__(self,\n",
        "                 n_epochs, \n",
        "                 batch_size, \n",
        "                 learning_rate,\n",
        "                 input_shape,\n",
        "                 logdir=None,\n",
        "                 deeper=False):\n",
        "        \n",
        "        \n",
        "        self.n_epochs = n_epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.input_shape = input_shape\n",
        "        if logdir is None:\n",
        "            self.logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "        else:\n",
        "            self.logdir = os.path.join(\"logs\", logdir)\n",
        "        \n",
        "        if deeper:\n",
        "            self._build_deeper_model()\n",
        "            \n",
        "        else:\n",
        "            self._build_model()\n",
        "\n",
        "    def _build_model(self):\n",
        "        \n",
        "        self.model = tf.keras.models.Sequential([\n",
        "            tf.keras.layers.Input(shape=self.input_shape),\n",
        "            tf.keras.layers.Flatten(),\n",
        "            tf.keras.layers.Dense(512, activation='relu', name=\"fc1\"),\n",
        "            tf.keras.layers.Dropout(0.2),\n",
        "            tf.keras.layers.Dense(10, activation='softmax', name=\"predictions\")\n",
        "            ])\n",
        "        print(self.input_shape)\n",
        "        print(\"Model build\")\n",
        "        \n",
        "    def _build_deeper_model(self):\n",
        "        # Start of modified section\n",
        "        # Model similar to LeNet-5 (but it doesn't seem to perform very well)\n",
        "        self.model = tf.keras.models.Sequential([\n",
        "            tf.keras.layers.Input(shape=self.input_shape),\n",
        "            tf.keras.layers.Conv2D(filters=6, kernel_size=(5, 5), activation='relu'),\n",
        "            tf.keras.layers.AveragePooling2D(),\n",
        "            tf.keras.layers.Conv2D(filters=16, kernel_size=(5, 5), activation='relu'),\n",
        "            tf.keras.layers.AveragePooling2D(),\n",
        "            tf.keras.layers.Flatten(),\n",
        "            tf.keras.layers.Dense(units=120, activation='relu'),\n",
        "            tf.keras.layers.Dense(units=84, activation='relu'),\n",
        "            tf.keras.layers.Dense(units=10, activation = 'softmax')\n",
        "        ])\n",
        "        print(\"Deeper model build\")\n",
        "        # End of modified section\n",
        "\n",
        "            \n",
        "    def train_model(self, x_train, y_train):\n",
        "        \n",
        "        x_train, y_train, x_val, y_val = self._split_validation_data(x_train, \n",
        "                                                                     y_train, \n",
        "                                                                     0.1)\n",
        "\n",
        "        \n",
        "        optimizer = tf.keras.optimizers.Adam(lr=self.learning_rate)\n",
        "        \n",
        "        self.model.compile(optimizer=optimizer,\n",
        "                           loss='sparse_categorical_crossentropy',\n",
        "                           metrics=['accuracy'])\n",
        "        \n",
        "        \n",
        "        tensorboard_callback = tf.keras.callbacks.TensorBoard(self.logdir, \n",
        "                                                              histogram_freq=1,\n",
        "                                                              write_images=True, \n",
        "                                                              write_grads=True)\n",
        "                                                              \n",
        "        # Keras API for training\n",
        "        # Start of modified section\n",
        "        self.model.fit(x_train, y_train,\n",
        "                       batch_size=64,\n",
        "                       epochs=10,\n",
        "                       validation_data=(x_val, y_val))\n",
        "        # End of modified section\n",
        "    \n",
        "    @staticmethod\n",
        "    def _split_validation_data(x, y, validation_split):\n",
        "        rand_indexes = np.random.permutation(x.shape[0])\n",
        "        x = x[rand_indexes]\n",
        "        y = y[rand_indexes]\n",
        "        x_validation = x[:int(len(x) * validation_split)]\n",
        "        y_validation = y[:int(len(x) * validation_split)]\n",
        "        x_train = x[int(len(x) * validation_split):]\n",
        "        y_train = y[int(len(x) * validation_split):]\n",
        "        return x_train, y_train, x_validation, y_validation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FnUwGOj-ljBk"
      },
      "source": [
        "### Some plot functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WeoSbE_UlBzD",
        "colab": {}
      },
      "source": [
        "class MyPlot():\n",
        "    def __init__(self,\n",
        "                 nrows, \n",
        "                 ncols, \n",
        "                 figsize):\n",
        "        self.fig, self.axes = plt.subplots(nrows=nrows, \n",
        "                                           ncols=ncols, \n",
        "                                           figsize=figsize)\n",
        "\n",
        "        \n",
        "def my_histogram(ax, data, color, title=None, rwidth=None, log=True, bins=25, align='mid', density=None):\n",
        "    ax.hist(data, color=color, log=log, bins=bins, edgecolor='black', linewidth=1.2, rwidth=rwidth, align=align, density=density);\n",
        "    ax.set_title(title);\n",
        "\n",
        "    \n",
        "def plot_random_images(images,\n",
        "                       means,\n",
        "                       labels=[],\n",
        "                       examples=16, \n",
        "                       fig_suptitle=None, \n",
        "                       figsize=(8,8), \n",
        "                       fpath=None, \n",
        "                       imgs_index=None):\n",
        "    \n",
        "    if imgs_index is None:\n",
        "        imgs_index = np.random.choice(np.arange(len(images)), examples, replace=False)\n",
        "    plot = MyPlot(int(examples/np.sqrt(examples)), int(examples/np.sqrt(examples)), figsize=figsize)\n",
        "    plot.axes = plot.axes.ravel()\n",
        "\n",
        "    for idx, _ in enumerate(plot.axes):\n",
        "        X_norm = images[imgs_index[idx]] * 255\n",
        "        X = np.zeros(X_norm.shape, dtype=np.uint8)\n",
        "        if len(X.shape) >= 3:\n",
        "            for i in range(X.shape[-1]):\n",
        "                X[:, :, i] = X_norm[:, :, i] + means[i]\n",
        "                plot.axes[idx].imshow(X)\n",
        "        else:\n",
        "            X = X_norm + means\n",
        "            plot.axes[idx].imshow(X, cmap=\"gray\")\n",
        "        plot.axes[idx].axis('off')\n",
        "        if len(labels) > 0:\n",
        "            plot.axes[idx].set_title(labels[imgs_index[idx]], fontsize=16, color=\"white\")\n",
        "        plot.fig.suptitle(fig_suptitle, fontsize=16)\n",
        "    if fpath:\n",
        "        plot.fig.savefig(fpath)\n",
        "        \n",
        "\n",
        "def compute_confusion_matrix(y_true, y_false):\n",
        "    cm = confusion_matrix(y_true, y_false)\n",
        "    return cm\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(cm,\n",
        "                          labels_categorical,\n",
        "                          title=None,\n",
        "                          normalize=False, \n",
        "                          figsize=(8, 8)):\n",
        "    \n",
        "    plot = MyPlot(1, 1 ,figsize=figsize)\n",
        "    ax = plot.axes\n",
        "    \n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "    else:\n",
        "        cm = cm\n",
        "    ax.imshow(cm, interpolation='nearest', cmap=plt.cm.get_cmap(\"Greys\"))\n",
        "\n",
        "    tick_marks = np.arange(len(labels_categorical))\n",
        "    ax.set_xticks(tick_marks)\n",
        "    ax.set_xticklabels(labels_categorical, fontsize=16, color=\"white\")\n",
        "    ax.set_yticks(tick_marks)\n",
        "    ax.set_yticklabels(labels_categorical, fontsize=16, color=\"white\")\n",
        "\n",
        "    fmt = '.2f'\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        if np.isnan(cm[i, j]):\n",
        "            cm[i, j] = 0\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        ax.text(j, i, format(cm[i, j], fmt),\n",
        "                horizontalalignment=\"center\",\n",
        "                color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    ax.set_ylabel('True label', fontsize=16, color=\"white\")\n",
        "    ax.set_xlabel('Predicted label', fontsize=16, color=\"white\")\n",
        "    ax.xaxis.set_tick_params(rotation=45)\n",
        "\n",
        "    if title:\n",
        "        ax.set_title(title)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FWCss-VqkdQ0"
      },
      "source": [
        "## Download the FashionMNIST dataset and preprocess it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5OyrD75ZkeTX",
        "colab": {}
      },
      "source": [
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "print(\"Train dataset shape:\\n\\t Images {}\".format(x_train.shape))\n",
        "print(\"\\t Labels {}\".format(y_train.shape))\n",
        "\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "x_train_mean = np.mean(x_train, axis=(0, 1, 2))\n",
        "print(\"Mean: {:.3f}\".format(x_train_mean))\n",
        "x_train = (x_train - x_train_mean)/255.0\n",
        "x_test = (x_test - x_train_mean)/255.0\n",
        "\n",
        "categorical_labels = [class_names[label] for label in y_train]\n",
        "plot_random_images(x_train, x_train_mean, labels=categorical_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "epDj9hnJmkg1"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QEEcRWs7mmpZ",
        "colab": {}
      },
      "source": [
        "my_net = MyNet(n_epochs=20,\n",
        "               batch_size=64, \n",
        "               learning_rate=0.001, # 0.01\n",
        "               input_shape=(28, 28, 1),\n",
        "               deeper=True, \n",
        "               logdir=\"fc_0-001\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XqIucF9SGBIb",
        "colab": {}
      },
      "source": [
        "my_net.train_model(np.expand_dims(x_train, axis=-1), y_train)\n",
        "loss, acc = my_net.model.evaluate(np.expand_dims(x_test, axis=-1), y_test, verbose=False);\n",
        "predictions = my_net.model.predict(np.expand_dims(x_test, axis=-1))\n",
        "print(\"Test done!\\n\\tMean accuracy: {}\\n\\tLoss: {}\".format(acc, loss))\n",
        "\n",
        "cm = compute_confusion_matrix(y_test, np.argmax(predictions, axis=1))\n",
        "plot_confusion_matrix(cm, class_names, normalize=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vs_NH_1PF1a7"
      },
      "source": [
        "## Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qnNE57MWs89r",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir logs/fc_0-001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JWqkqFEWF8Ge"
      },
      "source": [
        "## Biological case study"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "F70yfLNuGGj9"
      },
      "source": [
        "### Load data from gdrive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4iIKFL25GL2v",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")\n",
        "\n",
        "root_path = \"gdrive/My Drive/Datasets/BIOSTEC_2018\"\n",
        "x_train_path = os.path.join(root_path, \"train\", \"images.npy\")\n",
        "y_train_path = os.path.join(root_path, \"train\", \"labels.npy\")\n",
        "x_test_path = os.path.join(root_path, \"test\", \"images.npy\")\n",
        "y_test_path = os.path.join(root_path, \"test\", \"labels.npy\")\n",
        "\n",
        "x_train = np.load(x_train_path)\n",
        "y_train_categorical = np.load(y_train_path)\n",
        "x_test = np.load(x_test_path)\n",
        "y_test_categorical = np.load(y_test_path)\n",
        "\n",
        "print(\"Train dataset:\\n\\tImages: {}\\n\\tLabels: {}\".format(x_train.shape, y_train_categorical.shape))\n",
        "print(\"Test dataset:\\n\\tImages: {}\\n\\tLabels: {}\".format(x_test.shape, y_test_categorical.shape))\n",
        "\n",
        "# Resize images to speed-up training\n",
        "x_train = np.asarray([cv2.resize(image, (64, 64), \n",
        "                                 interpolation = cv2.INTER_CUBIC) for image in x_train])\n",
        "x_test = np.asarray([cv2.resize(image, (64, 64), \n",
        "                                interpolation = cv2.INTER_CUBIC) for image in x_test])\n",
        "\n",
        "# Convert labels to int\n",
        "conversions = dict()\n",
        "conversions['AC'] = 0\n",
        "conversions['H'] = 1\n",
        "conversions['AD'] = 2\n",
        "y_train = np.asarray([conversions[label] for label in y_train_categorical])\n",
        "y_test = np.asarray([conversions[label] for label in y_test_categorical])\n",
        "\n",
        "# Normalization\n",
        "# Start of modified section\n",
        "# I cannot do this part, I don't have the dataset\n",
        "scaler = StandardScaler()\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_test = scaler.transform(x_test)\n",
        "# End of modified section\n",
        "\n",
        "plot_random_images(x_train, x_train_mean, labels=y_train_categorical)\n",
        "\n",
        "# Start of modified section\n",
        "my_net = MyNet(n_epochs=20,\n",
        "               batch_size=64, \n",
        "               learning_rate=0.001, # 0.01\n",
        "               input_shape=(64, 64, 1),\n",
        "               deeper=True, \n",
        "               logdir=\"fc_0-002\")\n",
        "# End of modified section\n",
        "\n",
        "my_net.train_model(x_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w11clWXffZFp",
        "colab": {}
      },
      "source": [
        "loss, acc = my_net.model.evaluate(x_test, y_test, verbose=False);\n",
        "predictions = my_net.model.predict(x_test)\n",
        "print(\"Test done!\\n\\tMean accuracy: {}\\n\\tLoss: {}\".format(acc, loss))\n",
        "\n",
        "cm = compute_confusion_matrix(y_test, np.argmax(predictions, axis=1))\n",
        "categorical_labels = ['AC', 'H', 'AD']\n",
        "plot_confusion_matrix(cm, categorical_labels, normalize=True, figsize=(4, 4))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}